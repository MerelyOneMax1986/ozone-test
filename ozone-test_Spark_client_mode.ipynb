{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kerberos init\n",
    "!chown jovyan /home/jovyan/testuser.keytab\n",
    "!chmod 400 /home/jovyan/testuser.keytab\n",
    "\n",
    "!klist -kte /home/jovyan/testuser.keytab\n",
    "!kinit -kt /home/jovyan/testuser.keytab testuser/scm@EXAMPLE.COM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Kubernetes client\n",
    "!pip install kubernetes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark conf setup\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = '/usr/local/spark/conf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client.rest import ApiException\n",
    "import json\n",
    "\n",
    "def get_spark_driver_ip_address():\n",
    "    import socket\n",
    "    hostname = socket.gethostname()\n",
    "    local_ip = socket.gethostbyname(hostname)\n",
    "    return local_ip\n",
    "\n",
    "def get_spark_driver_url():\n",
    "    return \"spark://jupyter:7077\"\n",
    "\n",
    "def create_service_endpoint(k8s_api_client, \n",
    "                   service_endpoint_name: str,\n",
    "                   pod_ip_address: str,\n",
    "                   labels: dict,\n",
    "                   port: str=7070,\n",
    "                   namespace: str='default'\n",
    "                  ):\n",
    "    \n",
    "    core_v1 = k8s_api_client\n",
    "\n",
    "    api_response = None\n",
    "    try:\n",
    "        api_response = core_v1.read_namespaced_endpoints(name=service_endpoint_name,\n",
    "                                                        namespace=namespace)\n",
    "    except ApiException as e:\n",
    "        if e.status != 404:\n",
    "            print(\"Unknown error: %s\" % e)\n",
    "            exit(1)\n",
    "\n",
    "    if not api_response:\n",
    "        service_endpoint_manifest = {\n",
    "                'apiVersion': 'v1',\n",
    "                'kind': 'Endpoints',\n",
    "                'metadata': {\n",
    "                    'labels': labels,\n",
    "                    'name': service_endpoint_name,\n",
    "                    'namespace': namespace\n",
    "                },\n",
    "                'subsets': [{\n",
    "                    'addresses': [{\n",
    "                        'ip': pod_ip_address\n",
    "                    }],\n",
    "                    'ports': [{\n",
    "                        'protocol': 'TCP',\n",
    "                        'port': port\n",
    "                    }]\n",
    "                }]\n",
    "            }\n",
    "\n",
    "        #print(f'SERVICE MANIFEST:\\n{service_endpoint_manifest}')\n",
    "\n",
    "        api_response = core_v1.create_namespaced_endpoints(body=service_endpoint_manifest, namespace=namespace)\n",
    "        #print(api_response)\n",
    "    return api_response\n",
    "\n",
    "def create_service(k8s_api_client, service_name: str,\n",
    "                   labels: dict,\n",
    "                   selector: dict,\n",
    "                   port: str=7070,\n",
    "                   target_port: str=7070,\n",
    "                   namespace: str='default'\n",
    "                  ):\n",
    "    \n",
    "    core_v1 = k8s_api_client\n",
    "\n",
    "    api_response = None\n",
    "    try:\n",
    "        api_response = core_v1.read_namespaced_service(name=service_name,\n",
    "                                                        namespace=namespace)\n",
    "    except ApiException as e:\n",
    "        if e.status != 404:\n",
    "            print(\"Unknown error: %s\" % e)\n",
    "            exit(1)\n",
    "\n",
    "    if not api_response:\n",
    "        service_manifest = {\n",
    "                'apiVersion': 'v1',\n",
    "                'kind': 'Service',\n",
    "                'metadata': {\n",
    "                    'labels': labels,\n",
    "                    'name': service_name,\n",
    "                    'namespace': namespace\n",
    "                },\n",
    "                'spec': {\n",
    "                    'clusterIP': 'None',\n",
    "                    'selector': selector,\n",
    "                    'ports': [{\n",
    "                        'protocol': 'TCP',\n",
    "                        'port': port,\n",
    "                        'targetPort': target_port\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        api_response = core_v1.create_namespaced_service(body=service_manifest, namespace=namespace)\n",
    "    return api_response\n",
    "\n",
    "def get_kubernetes_core_api():\n",
    "    from kubernetes import config\n",
    "    from kubernetes.client.api import core_v1_api\n",
    "\n",
    "    config.load_kube_config(config_file='/home/jovyan/.kube/config', context='spark')\n",
    "    core_v1 = core_v1_api.CoreV1Api()\n",
    "    return core_v1\n",
    "\n",
    "def create_spark_executor_template(cmd: list, \n",
    "                   labels: dict,\n",
    "                   volumes: [dict],\n",
    "                   volumeMounts: [dict],\n",
    "                   env: [dict],\n",
    "                   resources: dict,\n",
    "                   restartPolicy: str='Always'):\n",
    "    '''\n",
    "    This method returns a Spark executor pod manifest\n",
    "    '''\n",
    "    # Create pod manifest\n",
    "    pod_manifest = {\n",
    "            'apiVersion': 'v1',\n",
    "            'kind': 'Pod',\n",
    "            'metadata': {\n",
    "                'labels': labels,\n",
    "                #'name': pod_name\n",
    "            },\n",
    "            'spec': {\n",
    "                'containers': [{\n",
    "#                    'name': pod_name,\n",
    "#                    'args': cmd,\n",
    "                    'env': env,\n",
    "                    'resources': resources,\n",
    "                }],\n",
    "#                'tolerations': pod_tolerations,\n",
    "                'restartPolicy': restartPolicy,\n",
    "                'terminationGracePeriodSeconds': 0\n",
    "            }\n",
    "        }\n",
    "    with open('spark_executor_template.yaml', 'w') as outfile:\n",
    "        json.dump(pod_manifest, outfile)\n",
    "\n",
    "class SparkEnvironment:\n",
    "    _clustersize = 0\n",
    "    _config = None\n",
    "    _proxybase = None\n",
    "    _spark = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _startMaster():\n",
    "        spark_driver_host = 'jupyter'\n",
    "        spark_driver_port = 7077\n",
    "        spark_namespace = 'default'\n",
    "        spark_driver_ip_address = get_spark_driver_ip_address()\n",
    "        \n",
    "        v1 = get_kubernetes_core_api()\n",
    "        \n",
    "        create_service_endpoint(v1, service_endpoint_name=spark_driver_host,\n",
    "                    pod_ip_address=spark_driver_ip_address,\n",
    "                    labels={'name': spark_driver_host},\n",
    "                    port=spark_driver_port,\n",
    "                    namespace=spark_namespace\n",
    "                    ) \n",
    "                                \n",
    "        create_service(v1, service_name=spark_driver_host,\n",
    "                    labels={'name': spark_driver_host},\n",
    "                    selector={},\n",
    "                    port=spark_driver_port,\n",
    "                    target_port=spark_driver_port,\n",
    "                    namespace=spark_namespace\n",
    "                    )\n",
    "\n",
    "    def _conf( conf = None, debug = False ):\n",
    "        from pyspark import SparkConf\n",
    "        \n",
    "        if conf is None:\n",
    "            conf = SparkConf(False)\n",
    "        \n",
    "        if SparkEnvironment._clustersize >= 1:         \n",
    "            # Create Spark config for our Kubernetes based cluster manager\n",
    "            conf.setMaster(\"k8s://https://kubernetes.default:443\")\n",
    "            conf.setAppName(\"spark\")\n",
    "            conf.set(\"spark.kubernetes.namespace\", \"default\")\n",
    "            conf.set(\"spark.executor.instances\", SparkEnvironment._clustersize)\n",
    "            conf.set(\"spark.kubernetes.executor.request.cores\", \"3\")\n",
    "            conf.set(\"spark.executor.memory\", \"2g\")\n",
    "        \n",
    "            conf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "            conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "            conf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "            conf.set(\"spark.kubernetes.context\", \"spark\")\n",
    "            conf.set(\"spark.kubernetes.trust.certificates\", \"true\")            \n",
    "            conf.set(\"spark.kubernetes.container.image\", \"buslovaev/pyspark-ozone:v3.2.1\")\n",
    "            conf.set(\"spark.kubernetes.kerberos.krb5.path\", \"/opt/spark/conf/krb5.conf\")\n",
    "            \n",
    "            conf.set(\"spark.kerberos.keytab\", \"/home/jovyan/testuser.keytab\")\n",
    "            conf.set(\"spark.kerberos.principal\", \"testuser/scm@EXAMPLE.COM\")\n",
    "            conf.set(\"spark.driver.memory\", \"2g\")\n",
    "            \n",
    "            conf.set(\"spark.driver.host\", 'jupyter.default')\n",
    "            conf.set(\"spark.driver.port\", 7077)\n",
    "            if debug:\n",
    "                conf.set(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dlog4jspark.root.logger=DEBUG,console -Djava.security.krb5.conf=/etc/krb5.conf\")\n",
    "                conf.set(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true -Dlog4jspark.root.logger=DEBUG,console\")\n",
    "                conf.set(\"spark.executor.extraJavaOptions\", f\"-Dlog4j.configuration=file:/opt/spark/conf/log4j.properties -Djava.security.krb5.conf=/opt/spark/conf/krb5.conf\")\n",
    "                \n",
    "                create_spark_executor_template(cmd=[],                   \n",
    "                   labels={\"type\": \"spark-worker\"},\n",
    "                   volumes={},\n",
    "                   volumeMounts={},\n",
    "                   env=[{'name': 'HADOOP_CONF_DIR', 'value': '/opt/spark/conf'}, {'name': 'HADOOP_USER_NAME', 'value': 'testuser'}, {'name': 'HADOOP_JAAS_DEBUG', 'value': 'true'}, {'name': 'HADOOP_OPTS', 'value': '-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug'} ],\n",
    "                   resources={},                            \n",
    "                   restartPolicy='Always')\n",
    "                \n",
    "                conf.set(\"spark.kubernetes.executor.podTemplateFile\", \"spark_executor_template.yaml\")\n",
    "        \n",
    "        SparkEnvironment._config = conf\n",
    "        return conf\n",
    "\n",
    "    @staticmethod\n",
    "    def _master(clusterSize=0, debug=False):\n",
    "        SparkEnvironment._clustersize = clusterSize\n",
    "        if SparkEnvironment._clustersize >= 1:\n",
    "            SparkEnvironment._startMaster()\n",
    "            return get_spark_driver_url()\n",
    "        elif SparkEnvironment._clustersize == 0:\n",
    "            return \"local[*]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def runSparkSession(clusterSize=0 , app=\"my app\", conf=None, debug=False):\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.context import SparkContext\n",
    "        from pyspark.conf import SparkConf\n",
    "\n",
    "        master = SparkEnvironment._master(clusterSize, debug)\n",
    "        config = SparkEnvironment._conf(conf, debug)\n",
    "        SparkEnvironment._spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(app) \\\n",
    "            .master(master) \\\n",
    "            .config(conf=config)\\\n",
    "            .getOrCreate()\n",
    "        # DEBUG FOR SPARK MASTER\n",
    "        if debug:\n",
    "            SparkEnvironment._spark.sparkContext.setLogLevel('DEBUG')    \n",
    "        return SparkEnvironment._spark\n",
    "    \n",
    "    @staticmethod\n",
    "    def stopSparkSession():\n",
    "        try:\n",
    "            SparkEnvironment._spark.stop()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#8 Run Spark cluster in a client mode\n",
    "SparkEnvironment.stopSparkSession()\n",
    "spark = SparkEnvironment.runSparkSession(1, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sample data to Ozone bare metal secured\n",
    "ozone_data_path='/volume1/bucket1/test2'\n",
    "spark.createDataFrame(\n",
    "        [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
    ").write.parquet(ozone_data_path, mode=\"overwrite\")\n",
    "\n",
    "df7 = spark.read.parquet(ozone_data_path)\n",
    "df7.show()\n",
    "df7 = spark.read.parquet(ozone_data_path).to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop Spark cluster\n",
    "SparkEnvironment.stopSparkSession()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "boosting",
   "language": "python",
   "name": "boosting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
